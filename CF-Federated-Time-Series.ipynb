{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41567fa9-9d1e-428a-a33f-6fb7105bf9b3",
   "metadata": {},
   "source": [
    "<h1>Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting </h1> \n",
    "\n",
    "Dataset link: https://archive.ics.uci.edu/dataset/501/beijing+multi+site+air+quality+data\n",
    "\n",
    "How to run:\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Run each of the steps in the following order: \n",
    "\n",
    "1. Imports & Global Config (modify the hyperparameters)\n",
    "\n",
    "2. Data Loading, Basic Cleaning, Feature Engineering\n",
    "\n",
    "3. Splits, Global Robust Normalization, Lagged Samples\n",
    "\n",
    "4. Dataset & DataLoaders\n",
    "\n",
    "5. Model & Utilities (Base training or loading)\n",
    "\n",
    "6. Train Offline Base Model\n",
    "\n",
    "7. Initialize Replay Buffers & Fisher\n",
    "\n",
    "8. Continual Online learning function\n",
    "\n",
    "9. Run Ablation & Evaluate all the proposed methods (Naive, Replay, KD, EWC, O-EWC, SI)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c56be-fb88-4613-9f88-19b26b9b4974",
   "metadata": {},
   "source": [
    "<h2> 1. Imports & Global Config </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9411af38-1c78-41be-8596-6533bea4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Benchmarking Catastrophic Forgetting in Federated TS Forecasting\n",
    "# CONFIG + PRACTICAL TIPS FOR REPRO USERS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 6), \"axes.grid\": True})\n",
    "\n",
    "# ---- Device & deterministic seeds ----\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_all_seeds(seed: int = 42) -> None:\n",
    "    import random\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(42)\n",
    "\n",
    "# ---- Paths ----\n",
    "# TIP: We intentionally do NOT commit raw data. Put CSVs under this folder.\n",
    "# If you change repo layout, update this path accordingly.\n",
    "DATASET_DIR = Path(\"../dataset/AirQuality\")\n",
    "\n",
    "# TIP: Expected 12 station files (UCI dataset). If you subset to run faster,\n",
    "# you can comment out a few lines below‚Äîthe pipeline adapts automatically.\n",
    "DATASET_FILES = [\n",
    "    \"Aotizhongxin.csv\",\"Changping.csv\",\"Dingling.csv\",\"Dongsi.csv\",\"Guanyuan.csv\",\"Gucheng.csv\",\n",
    "    \"Huairou.csv\",\"Nongzhanguan.csv\",\"Shunyi.csv\",\"Tiantan.csv\",\"Wanliu.csv\",\"Wanshouxigong.csv\"\n",
    "]\n",
    "\n",
    "# ------------------- Forecasting Task & Shapes -----------------\n",
    "# TARGET_COL: what we predict. Options commonly tested: \"WSPM\", \"PM2.5\", \"TEMP\".\n",
    "# TIP: If you switch TARGET_COL, you do NOT need to change features below;\n",
    "# the pipeline includes target in the model input by default.\n",
    "TARGET_COL = \"WSPM\"   # e.g., set to \"PM2.5\" or \"TEMP\" to reproduce other tasks\n",
    "\n",
    "# N_LAGS: input window length (hours). PRED_LEN: how many steps ahead (hours) to predict.\n",
    "# ‚Ä¢ Larger N_LAGS gives model more context but increases memory/compute.\n",
    "# ‚Ä¢ Larger PRED_LEN makes the task harder (multi-step horizon).\n",
    "N_LAGS   = 12\n",
    "PRED_LEN = 6\n",
    "\n",
    "# SPLIT_RATIO within each continual task: 80% train / 20% test by default.\n",
    "# TIP: Keep this ratio fixed when comparing methods; changing it shifts metrics.\n",
    "SPLIT_RATIO = 0.8      \n",
    "\n",
    "# Batch size: set lower on CPU or memory-constrained GPUs.\n",
    "BATCH_SIZE  = 32\n",
    "\n",
    "# ------------------- Base Federated Pretraining ----------------\n",
    "# Base FL pretraining stabilizes the initial model before continual learning.\n",
    "# ‚Ä¢ NUM_ROUNDS_BASE: communication rounds over base data (increase for better base).\n",
    "# ‚Ä¢ LR_BASE, LOCAL_EPOCHS_BASE: local optimization settings per client.\n",
    "# TIP: If you are doing a quick smoke run, reduce NUM_ROUNDS_BASE (e.g., 10‚Äì20).\n",
    "NUM_ROUNDS_BASE  = 200     # paper-grade: can be 200‚Äì500+\n",
    "LR_BASE          = 1e-5\n",
    "LOCAL_EPOCHS_BASE = 1\n",
    "\n",
    "# --------------------- Continual Learning Loop -----------------\n",
    "# NUM_ROUNDS_CL: rounds per task during continual learning.\n",
    "# LR: learning rate used during CL (can differ from base LR).\n",
    "# TIP: For fast runs, set NUM_ROUNDS_CL=5‚Äì10; for paper fidelity, use 20‚Äì30+.\n",
    "NUM_ROUNDS_CL = 25\n",
    "LOCAL_EPOCHS  = 1\n",
    "LR            = 1e-5\n",
    "\n",
    "# ----------------------- Method Coefficients -------------------\n",
    "# These are regularization/auxiliary-loss weights. Defaults reflect paper tuning.\n",
    "# ‚Ä¢ SI_COEFF: Synaptic Intelligence Œª\n",
    "# ‚Ä¢ KD_COEFF: Distillation strength (if KD is enabled)\n",
    "# ‚Ä¢ EWC_COEFF: Classic EWC penalty weight\n",
    "# ‚Ä¢ ONLINE_EWC_COEFF: Online EWC (moving Fisher) penalty weight\n",
    "# ‚Ä¢ REPLAY_COEFF: Weight on replay loss when buffers are used\n",
    "# ‚Ä¢ REPLAY_RATIO: Fraction of task samples selected into buffer via KMeans\n",
    "# ‚Ä¢ BUFFER_CAPACITY: Max stored (x,y) sequences per client\n",
    "#\n",
    "# Tuning guidance:\n",
    "#  - If training diverges, lower LR and/or the largest active coefficient.\n",
    "#  - If AF (forgetting) is high, try increasing EWC/SI or enabling replay.\n",
    "#  - If AP (plasticity) is too low (model underfits new tasks), reduce regularization.\n",
    "SI_COEFF          = 14\n",
    "KD_COEFF          = 120\n",
    "EWC_COEFF         = 1e8\n",
    "ONLINE_EWC_COEFF  = 1.3e6\n",
    "REPLAY_COEFF      = 0.8\n",
    "REPLAY_RATIO      = 0.15\n",
    "BUFFER_CAPACITY   = 100000\n",
    "\n",
    "\n",
    "# -------------------------- Practical Notes --------------------\n",
    "# ‚Ä¢ GPU vs CPU: All code runs on CPU but will be slow. If GPU present, it‚Äôs auto-used.\n",
    "# ‚Ä¢ Memory: KMeans for replay runs on flattened sequences; REPLAY_RATIO scales time/memory.\n",
    "#   Set REPLAY_RATIO=0 to disable replay entirely (or choose the \"Naive\"/\"kd\"/\"ewc\" modes).\n",
    "# ‚Ä¢ Repro table: Our evaluation matches the paper‚Äôs ‚Äúlegacy‚Äù protocol (P_{N,1..N-1} for AvgPerf,\n",
    "#   AF via P_{N,j}-P_{j,j}, AP via diag mean). For ‚Äústrict‚Äù CL metrics with best-past baseline,\n",
    "#   see the alternative evaluator in comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711e96d-3d9d-4022-9929-1678d15d4f54",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h1>2. Data Loading, Basic Cleaning, Feature Engineering</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4f3583d-c176-4839-b920-1170985fc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Load raw CSVs ‚Üí dict of per-station DataFrames\n",
    "# -------------------------------------------------\n",
    "# Tips for reusers:\n",
    "# ‚Ä¢ We don't ship the data. Ensure DATASET_DIR points to the folder holding the 12 CSVs.\n",
    "# ‚Ä¢ If you subset stations for a quick run, just remove their filenames from DATASET_FILES.\n",
    "# ‚Ä¢ All timestamps are interpreted as naive local time; we sort by index immediately.\n",
    "\n",
    "def load_clients(dataset_dir: Path, files: List[str]) -> Dict[str, pd.DataFrame]:\n",
    "    clients = {}\n",
    "    for f in files:\n",
    "        df = pd.read_csv(dataset_dir / f)\n",
    "        df[\"time\"] = pd.to_datetime(df[[\"year\",\"month\",\"day\",\"hour\"]])\n",
    "        df = df.set_index(\"time\").sort_index()\n",
    "        clients[f.replace(\".csv\",\"\")] = df\n",
    "    return clients\n",
    "\n",
    "client_dfs = load_clients(DATASET_DIR, DATASET_FILES)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Missing-value handling\n",
    "# -------------------------------------------------\n",
    "# Strategy:\n",
    "# ‚Ä¢ Numeric columns: time interpolation (linear in time) with both-sided fill at edges.\n",
    "# ‚Ä¢ Categorical 'wd' (wind direction): forward/backward fill.\n",
    "# ‚Ä¢ Finally, drop any row still containing NaNs (rare after the two steps).\n",
    "# Why this matters:\n",
    "# ‚Ä¢ Interpolated gaps keep temporal coherence and avoid data leakage from future tasks.\n",
    "# ‚Ä¢ Strict drop at the end prevents NaNs from breaking metrics later.\n",
    "\n",
    "def smart_imputation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    cont_cols = out.select_dtypes(include=[np.number]).columns\n",
    "    out[cont_cols] = out[cont_cols].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "    if \"wd\" in out.columns:\n",
    "        out[\"wd\"] = out[\"wd\"].ffill().bfill()\n",
    "    out = out.dropna()\n",
    "    return out\n",
    "\n",
    "client_dfs = {k: smart_imputation(v) for k, v in client_dfs.items()}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Wind direction encoding (circular features)\n",
    "# -------------------------------------------------\n",
    "# We map 16-point compass directions to angles (deg), then to sin/cos.\n",
    "# Unknown tokens (if any) map to angle 0 (calm) to avoid NaNs.\n",
    "\n",
    "WD2ANG = {'N':0,'NNE':22.5,'NE':45,'ENE':67.5,'E':90,'ESE':112.5,'SE':135,'SSE':157.5,\n",
    "          'S':180,'SSW':202.5,'SW':225,'WSW':247.5,'W':270,'WNW':292.5,'NW':315,'NNW':337.5}\n",
    "\n",
    "def encode_wind_direction(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if \"wd\" in out.columns:\n",
    "        ang = out[\"wd\"].map(WD2ANG).fillna(0.0)\n",
    "        rad = np.deg2rad(ang)\n",
    "        out[\"wd_sin\"] = np.sin(rad); out[\"wd_cos\"] = np.cos(rad)\n",
    "    return out\n",
    "\n",
    "client_dfs = {k: encode_wind_direction(v) for k, v in client_dfs.items()}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Cyclical time features (hour/month/day)\n",
    "# -------------------------------------------------\n",
    "# Tip:\n",
    "# ‚Ä¢ We retain the original integer columns elsewhere for potential ablations.\n",
    "# ‚Ä¢ If your downstream only uses the encodings, that‚Äôs fine ‚Äî keeping both is harmless.\n",
    "\n",
    "def add_time_cycles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if all(c in out.columns for c in [\"hour\",\"month\",\"day\"]):\n",
    "        out[\"hour_sin\"]  = np.sin(2*np.pi*out[\"hour\"]/24)\n",
    "        out[\"hour_cos\"]  = np.cos(2*np.pi*out[\"hour\"]/24)\n",
    "        out[\"month_sin\"] = np.sin(2*np.pi*out[\"month\"]/12)\n",
    "        out[\"month_cos\"] = np.cos(2*np.pi*out[\"month\"]/12)\n",
    "        out[\"day_sin\"]   = np.sin(2*np.pi*out[\"day\"]/31)\n",
    "        out[\"day_cos\"]   = np.cos(2*np.pi*out[\"day\"]/31)\n",
    "    return out\n",
    "\n",
    "client_dfs = {k: add_time_cycles(v) for k, v in client_dfs.items()}\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Final feature set\n",
    "# -------------------------------------------------\n",
    "# Include 'year' to allow robust global [1,99] percentile normalization per feature.\n",
    "# Tip for custom tasks:\n",
    "# ‚Ä¢ You can change TARGET_COL elsewhere (e.g., \"PM2.5\") without modifying this list;\n",
    "#   the modeling code handles inclusion/exclusion of target correctly.\n",
    "\n",
    "FEATURES = [\n",
    "    \"year\",\n",
    "    \"PM2.5\",\"PM10\",\"SO2\",\"NO2\",\"CO\",\"O3\",\n",
    "    \"TEMP\",\"PRES\",\"DEWP\",\"RAIN\",\"WSPM\",\n",
    "    \"wd_sin\",\"wd_cos\",\n",
    "    \"hour_sin\",\"hour_cos\",\"month_sin\",\"month_cos\",\"day_sin\",\"day_cos\",\n",
    "]\n",
    "client_dfs = {k: v[[c for c in FEATURES if c in v.columns]].copy() for k, v in client_dfs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5dc876-7ab8-4481-b6a1-90593e2d9571",
   "metadata": {},
   "source": [
    " <h1>3. Splits, Global Robust Normalization, Lagged Samples</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e27cafac-465e-4cb5-ba97-f90874bea4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Chronological splits, global robust normalization, lagged samples\n",
    "# -------------------------------------------------------------------\n",
    "# Why this matters:\n",
    "# ‚Ä¢ Splits follow the paper‚Äôs protocol: a base window, a tiny base_test day, then 11 tasks.\n",
    "# ‚Ä¢ GLOBAL_BOUNDS is computed once across all clients to avoid data leakage across tasks.\n",
    "# ‚Ä¢ create_lagged_samples builds (X,y) windows with N_LAGS history and PRED_LEN horizon.\n",
    "\n",
    "# ---- Task schedule (paper protocol) ----\n",
    "TASK_RANGES = [\n",
    "    (\"2014-05-05\",\"2014-08-06\"),(\"2014-08-07\",\"2014-11-06\"),(\"2014-11-07\",\"2015-02-02\"),\n",
    "    (\"2015-02-03\",\"2015-05-04\"),(\"2015-05-05\",\"2015-08-06\"),(\"2015-08-07\",\"2015-11-06\"),\n",
    "    (\"2015-11-07\",\"2016-02-02\"),(\"2016-02-03\",\"2016-05-04\"),(\"2016-05-05\",\"2016-08-06\"),\n",
    "    (\"2016-08-07\",\"2016-11-06\"),(\"2016-11-07\",\"2017-02-02\"),\n",
    "]\n",
    "\n",
    "def split_ranges(df: pd.DataFrame) -> Tuple[pd.DataFrame,pd.DataFrame,List[pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "    Returns base, base_test, and a list of task DataFrames (chronological).\n",
    "    Guardrails:\n",
    "    ‚Ä¢ Drops empty task windows (rare per station if raw dumps differ).\n",
    "    ‚Ä¢ Keeps exact time bounds to match the paper.\n",
    "    \"\"\"\n",
    "    base = df.loc[\"2013-05-01\":\"2014-05-03 23:00:00\"]\n",
    "    base_test = df.loc[\"2014-05-04\":\"2014-05-04 23:00:00\"]\n",
    "    tasks = [df.loc[s:e].copy() for (s,e) in TASK_RANGES if not df.loc[s:e].empty]\n",
    "    return base, base_test, tasks\n",
    "\n",
    "# Global robust [1,99] percentile bounds across all clients\n",
    "def compute_global_bounds(clients: Dict[str, pd.DataFrame], cols: List[str]) -> Dict[str, Tuple[float,float]]:\n",
    "    percs = {c: [] for c in cols}\n",
    "    for _, df in clients.items():\n",
    "        for c in cols:\n",
    "            if c in df and pd.api.types.is_numeric_dtype(df[c]):\n",
    "                arr = df[c].dropna().values\n",
    "                if arr.size:\n",
    "                    percs[c].append((np.percentile(arr,1), np.percentile(arr,99)))\n",
    "    bounds = {c: (min(x for x,_ in percs[c]), max(y for _,y in percs[c])) for c in cols if percs[c]}\n",
    "    return bounds\n",
    "\n",
    "GLOBAL_BOUNDS = compute_global_bounds(client_dfs, FEATURES)\n",
    "\n",
    "def normalize_df_globally(df: pd.DataFrame, bounds: Dict[str,Tuple[float,float]], ordered_cols: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "    Apply robust min‚Äìmax using precomputed global bounds.\n",
    "    ‚Ä¢ Column order is enforced by `ordered_cols` to keep model input consistent.\n",
    "    ‚Ä¢ If a column has no bounds (e.g., missing in some clients), it's left unchanged.\n",
    "    ‚Ä¢ Constant features (hi==lo) become 0.0.\n",
    "    \"\"\"\n",
    "    # keep only available columns in the defined order\n",
    "    cols = [c for c in ordered_cols if c in df.columns]\n",
    "    out = df[ordered_cols].copy()\n",
    "    for c in out.columns:\n",
    "        if c in bounds and pd.api.types.is_numeric_dtype(out[c]):\n",
    "            lo, hi = bounds[c]\n",
    "            out[c] = 0.0 if hi == lo else (out[c]-lo)/(hi-lo)\n",
    "            out[c] = out[c].clip(0,1)\n",
    "    return out\n",
    "\n",
    "def create_lagged_samples(df: pd.DataFrame, n_lags: int, pred_len: int, target_col: str = \"WSPM\",\n",
    "                          include_target_in_input: bool = True) -> Tuple[np.ndarray,np.ndarray]:\n",
    "\n",
    "        \"\"\"\n",
    "    Build overlapping windows:\n",
    "      X: [n_samples, n_lags, n_features], Y: [n_samples, pred_len]\n",
    "    ‚Ä¢ include_target_in_input=True matches our paper‚Äôs setup (target also in X).\n",
    "    ‚Ä¢ NaN windows are skipped defensively (should be rare post-cleaning).\n",
    "    \"\"\"\n",
    "    \n",
    "    cols = df.columns.tolist() if include_target_in_input else [c for c in df.columns if c != target_col]\n",
    "    Xv = df[cols].astype(np.float64).values\n",
    "    Yv = df[[target_col]].astype(np.float64).values\n",
    "    X, Y = [], []\n",
    "    for i in range(n_lags, len(df)-pred_len+1):\n",
    "        xw = Xv[i-n_lags:i]\n",
    "        yw = Yv[i:i+pred_len].flatten()\n",
    "        if np.isnan(xw).any() or np.isnan(yw).any(): \n",
    "            continue\n",
    "        X.append(xw); Y.append(yw)\n",
    "    return np.asarray(X), np.asarray(Y)\n",
    "\n",
    "# Build all lagged splits per client\n",
    "base_lagged, task_lagged = {}, {\"base_test\": {}}\n",
    "\n",
    "for client, df in client_dfs.items():\n",
    "    base, base_test, tasks = split_ranges(df)\n",
    "    base_n   = normalize_df_globally(base, GLOBAL_BOUNDS, FEATURES)\n",
    "    baseT_n  = normalize_df_globally(base_test, GLOBAL_BOUNDS, FEATURES)\n",
    "    Xb, yb   = create_lagged_samples(base_n, N_LAGS, PRED_LEN, TARGET_COL)\n",
    "    Xbt, ybt = create_lagged_samples(baseT_n, N_LAGS, PRED_LEN, TARGET_COL)\n",
    "\n",
    "    base_lagged[client] = {\"X\": Xb, \"y\": yb}\n",
    "    task_lagged[\"base_test\"][client] = {\"X\": Xbt, \"y\": ybt}\n",
    "\n",
    "    for t_idx, tdf in enumerate(tasks, 1):\n",
    "        tn = normalize_df_globally(tdf, GLOBAL_BOUNDS, FEATURES)\n",
    "        Xt, yt = create_lagged_samples(tn, N_LAGS, PRED_LEN, TARGET_COL)\n",
    "        task_lagged.setdefault(f\"task_{t_idx}\", {})[client] = {\"X\": Xt, \"y\": yt}\n",
    "\n",
    "\n",
    "NUM_TASKS = len([k for k in task_lagged if k.startswith(\"task_\")])\n",
    "assert NUM_TASKS > 0, \"No tasks found; check TASK_RANGES.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98894c24-48a0-427e-a8ef-cdfd4144f361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8b5088e-b2af-462f-b87a-481b61a0534b",
   "metadata": {},
   "source": [
    " <h1>4. Dataset & DataLoaders</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761d5bac-942e-40d7-a341-c9e26127509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Dataset & DataLoaders\n",
    "# ------------------------\n",
    "# Tips for readers:\n",
    "# ‚Ä¢ Determinism: we use a fixed torch.Generator for shuffling.\n",
    "# ‚Ä¢ Empty windows: if a (client, task) yields 0 samples after lagging, we still\n",
    "#   register an *empty* DataLoader so downstream code won‚Äôt KeyError; it will\n",
    "#   simply have 0 batches.\n",
    "# ‚Ä¢ You can increase num_workers once things run locally; we keep 0 in notebooks\n",
    "#   to avoid multiprocessing issues on some platforms.\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.as_tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.as_tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "client_dls = {\"base_train\": {}, \"base_test\": {}}\n",
    "for i in range(NUM_TASKS):\n",
    "    client_dls[f\"task_{i+1}_train\"] = {}\n",
    "    client_dls[f\"task_{i+1}_test\"]  = {}\n",
    "\n",
    "for c in base_lagged:\n",
    "    Xb, yb = base_lagged[c][\"X\"], base_lagged[c][\"y\"]\n",
    "    client_dls[\"base_train\"][c] = DataLoader(TimeSeriesDataset(Xb,yb), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    Xbt, ybt = task_lagged[\"base_test\"][c][\"X\"], task_lagged[\"base_test\"][c][\"y\"]\n",
    "    client_dls[\"base_test\"][c] = DataLoader(TimeSeriesDataset(Xbt,ybt), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    for i in range(NUM_TASKS):\n",
    "        key = f\"task_{i+1}\"\n",
    "        Xt, yt = task_lagged[key][c][\"X\"], task_lagged[key][c][\"y\"]\n",
    "        print(f\"üìé {c} - {key}: total = {len(Xt)}\")\n",
    "        \n",
    "        split = int(len(Xt)*SPLIT_RATIO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701d8b6-c336-4e6a-aaaf-a1b5ce68656a",
   "metadata": {},
   "source": [
    " <h1>5. Model & Utilities (Base training or loading)</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2277497a-dfca-4ff2-b64d-49307663b6b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5) Model & Utilities (base model + Fisher + replay buffer)\n",
    "# ----------------------------------------------------------\n",
    "# Tips for readers:\n",
    "# ‚Ä¢ LSTM head: we read the last hidden state only (common for seq2one / seq2many).\n",
    "# ‚Ä¢ Keep INPUT_DIM/HIDDEN_DIM/OUTPUT_DIM small for quick runs; scale up for paper runs.\n",
    "# ‚Ä¢ Fisher is estimated on a few batches (num_batches) for speed ‚Äî increase for stability.\n",
    "# ‚Ä¢ Replay buffer uses K-Means coresets over flattened windows; deterministic via random_state.\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "        \"\"\"\n",
    "    Minimal LSTM forecaster:\n",
    "      input:  [B, T, D]\n",
    "      output: [B, PRED_LEN]\n",
    "    We project only the last timestep‚Äôs hidden state.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int = 1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc   = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, x):  # x: [B, T, D]\n",
    "        out,_ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "        \n",
    "# Auto-detect model dims from prepared windows\n",
    "INPUT_DIM = base_lagged[next(iter(base_lagged))][\"X\"].shape[2]\n",
    "HIDDEN_DIM = 64\n",
    "OUTPUT_DIM = PRED_LEN\n",
    "\n",
    "class ReplayBuffer:\n",
    "        \"\"\"\n",
    "    Simple (x,y) FIFO replay buffer.\n",
    "    ‚Ä¢ Stores CPU tensors to keep GPU memory free.\n",
    "    ‚Ä¢ add(): ignores additions if capacity==0 (i.e., replay disabled).\n",
    "    ‚Ä¢ sample(): returns a batch; if buffer smaller than asked, returns all.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.buffer: List[Tuple[torch.Tensor,torch.Tensor]] = []\n",
    "    def add(self, x, y):\n",
    "        if self.capacity == 0: return\n",
    "        if len(self.buffer) >= self.capacity: self.buffer.pop(0)\n",
    "        self.buffer.append((x.detach().cpu(), y.detach().cpu()))\n",
    "    def sample(self, batch_size: int):\n",
    "        if not self.buffer: return (None,None)\n",
    "        idx = np.random.choice(len(self.buffer), min(batch_size, len(self.buffer)), replace=False)\n",
    "        X, y = zip(*[self.buffer[i] for i in idx])\n",
    "        return torch.stack(X), torch.stack(y)\n",
    "    def __len__(self): return len(self.buffer)\n",
    "\n",
    "def compute_ewc_fisher(model: nn.Module, loader: DataLoader, device=DEVICE, num_batches=10):\n",
    "        \"\"\"\n",
    "    Diagonal Fisher approximation via gradient^2 of MSE loss.\n",
    "    Notes:\n",
    "    ‚Ä¢ num_batches controls speed/variance trade-off; increase for more stable EWC.\n",
    "    ‚Ä¢ Model stays in eval() since we only need gradients of the loss w.r.t. parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval().to(device)\n",
    "    fis = {n: torch.zeros_like(p, device=device) for n,p in model.named_parameters()}\n",
    "    count = 0\n",
    "    for X,y in loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        model.zero_grad()\n",
    "        loss = nn.functional.mse_loss(model(X), y)\n",
    "        loss.backward()\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.grad is not None:\n",
    "                fis[n] += (p.grad.detach()**2)\n",
    "        count += 1\n",
    "        if count >= num_batches: break\n",
    "    for n in fis: fis[n] /= max(count,1)\n",
    "    return fis\n",
    "\n",
    "def init_base_buffer(predictor: nn.Module, loader: DataLoader, capacity=BUFFER_CAPACITY, sample_fraction=0.05):\n",
    "        \"\"\"\n",
    "    Build an initial replay coreset from base data via K-Means on flattened inputs.\n",
    "    Steps:\n",
    "      1) Collect all (X,y) from loader on CPU.\n",
    "      2) Flatten X: [N, T*D] for clustering (fast & simple).\n",
    "      3) Select closest sample to each centroid ‚áí diverse set.\n",
    "    \"\"\"\n",
    "    predictor.eval()\n",
    "    Xs, Ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            Xs.append(X.cpu()); Ys.append(y.cpu())\n",
    "    Xall = torch.cat(Xs, 0); Yall = torch.cat(Ys, 0)\n",
    "    embed = Xall.view(Xall.size(0), -1).numpy()\n",
    "    n_sel = max(1, int(sample_fraction * len(Xall)))\n",
    "    if n_sel >= len(embed):\n",
    "        sel_idx = np.arange(len(embed))\n",
    "    else:\n",
    "        km = KMeans(n_clusters=n_sel, random_state=42).fit(embed)\n",
    "        sel_idx = []\n",
    "        for k in range(n_sel):\n",
    "            ids = np.where(km.labels_ == k)[0]\n",
    "            if ids.size == 0: continue\n",
    "            D = np.linalg.norm(embed[ids] - km.cluster_centers_[k], axis=1)\n",
    "            sel_idx.append(ids[np.argmin(D)])\n",
    "    buf = ReplayBuffer(capacity)\n",
    "    for i in sel_idx: buf.add(Xall[i], Yall[i])\n",
    "    return buf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99ce83-bf82-4f5f-a902-f8866fddf183",
   "metadata": {
    "scrolled": true
   },
   "source": [
    " <h1>6. Base Model: Train (optional) or Load (default for speed) </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03c3248f-c09c-4aeb-900a-133ddd31a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Base Model: Train (optional) or Load\n",
    "# ---------------------------------------\n",
    "# Tips for readers:\n",
    "# ‚Ä¢ NUM_ROUNDS_BASE / LR_BASE / LOCAL_EPOCHS_BASE control the offline FL pretraining budget.\n",
    "# ‚Ä¢ We accumulate Synaptic Intelligence (SI) contributions per mini-batch:\n",
    "#       W += (theta_{t+1} - theta_t) * (-grad_t)\n",
    "#   and compute per-parameter omega:\n",
    "#       omega = W / ( (theta_T - theta_star)^2 + xi )\n",
    "#   with theta_star = params BEFORE local training on this client.\n",
    "# ‚Ä¢ FedAvg averages client weights each round.\n",
    "\n",
    "def train_federated_base(client_dls, input_dim, output_dim, hidden_dim=HIDDEN_DIM,\n",
    "                         num_rounds=NUM_ROUNDS_BASE, local_epochs=LOCAL_EPOCHS_BASE, lr=LR_BASE, device=DEVICE):\n",
    "    global_model = LSTMPredictor(input_dim, hidden_dim, output_dim).to(device)\n",
    "    gW = global_model.state_dict()\n",
    "\n",
    "    client_models = {}\n",
    "    si_omegas, si_prev, si_W = {}, {}, {}\n",
    "\n",
    "    for r in range(num_rounds):\n",
    "        local_states = []\n",
    "        for c, loader in client_dls[\"base_train\"].items():\n",
    "            m = LSTMPredictor(input_dim, hidden_dim, output_dim).to(device)\n",
    "            m.load_state_dict(gW); m.train()\n",
    "            opt = torch.optim.Adam(m.parameters(), lr=lr)\n",
    "            prev = {n: p.clone().detach() for n,p in m.named_parameters()}\n",
    "            \n",
    "            # SI accumulator W initialized to zeros (same shapes)\n",
    "            W    = {n: torch.zeros_like(p) for n,p in m.named_parameters()}\n",
    "            for _ in range(local_epochs):\n",
    "                for X,y in loader:\n",
    "                    X,y = X.to(device), y.to(device)\n",
    "                    opt.zero_grad(); loss = nn.functional.mse_loss(m(X), y)\n",
    "                    loss.backward(); opt.step()\n",
    "                    for n,p in m.named_parameters():\n",
    "                        if p.grad is not None:\n",
    "                            delta = p.detach() - prev[n]\n",
    "                            W[n] += delta * (-p.grad.detach())\n",
    "            local_states.append({k: v.detach().clone() for k,v in m.state_dict().items()})\n",
    "            client_models[c] = m\n",
    "            # SI omega\n",
    "            omega = {}\n",
    "            for n,p in m.named_parameters():\n",
    "                delta = p.detach() - prev[n]\n",
    "                omega[n] = W[n] / (delta.pow(2) + 1e-3)\n",
    "            si_omegas[c] = omega; si_prev[c] = {n: p.clone().detach() for n,p in m.named_parameters()}\n",
    "            si_W[c] = W\n",
    "\n",
    "        # FedAvg\n",
    "        newW = {k: sum(ls[k] for ls in local_states)/len(local_states) for k in local_states[0].keys()}\n",
    "        gW = newW\n",
    "\n",
    "    global_model.load_state_dict(gW)\n",
    "    return global_model, client_models, si_omegas, si_prev, si_W\n",
    "\n",
    "# --- Run base FL pretraining  ---\n",
    "base_model, base_clients, si_omegas, si_prev, si_W = train_federated_base(\n",
    "        client_dls, INPUT_DIM, OUTPUT_DIM, hidden_dim=HIDDEN_DIM, num_rounds=NUM_ROUNDS_BASE,\n",
    "        local_epochs=LOCAL_EPOCHS_BASE, lr=LR_BASE, device=DEVICE\n",
    "    )\n",
    "\n",
    "# Aliases used later in the notebook (keeps naming consistent)\n",
    "CLIENT_DATALOADERS = client_dls           # your dataloaders dict\n",
    "BASE_MODEL        = base_model            # global base model\n",
    "BASE_CLIENTS      = base_clients          # per-client base models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a0ca66-5575-4909-bfd6-e3fe52abad4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab36aef3-a830-44b1-a9b8-8df36f29867d",
   "metadata": {},
   "source": [
    " <h1>7. Initialize Replay Buffers & Fisher (once)  </h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed83aa8a-0007-47ba-9d14-3fae5b230e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Initialize Replay Buffers & Fisher (once)\n",
    "# -------------------------------------------\n",
    "# Tips for readers:\n",
    "# ‚Ä¢ Replay coreset is built from the BASE train split using K-Means over flattened windows.\n",
    "# ‚Ä¢ If REPLAY_RATIO is small and a client has very few samples, we keep all of them.\n",
    "# ‚Ä¢ Fisher is a diagonal approximation (grad^2 of MSE) computed on a few batches\n",
    "#   for speed. Increase the internal num_batches in `compute_ewc_fisher` for stability.\n",
    "\n",
    "initial_buffers = {}\n",
    "for c in client_dls[\"base_train\"]:\n",
    "    initial_buffers[c] = init_base_buffer(base_model, client_dls[\"base_train\"][c],\n",
    "                                          capacity=BUFFER_CAPACITY, sample_fraction=REPLAY_RATIO)\n",
    "\n",
    "# Compute per-client Fisher wrt their base reference\n",
    "fisher = {}\n",
    "for c in client_dls[\"base_train\"]:\n",
    "    ref = base_clients.get(c, base_model)  # fall back to global if no per-client base\n",
    "    fisher[c] = compute_ewc_fisher(ref, client_dls[\"base_train\"][c], device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d2ec9-5942-45df-b5f4-b236136ba4af",
   "metadata": {},
   "source": [
    "  <h1>8. Continual Learning </h1>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3afd36a2-7832-4b87-b1ac-6aac50a384da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continual_learning_hybrid(\n",
    "    client_dataloaders,\n",
    "    client_buffers,\n",
    "    base_predictor,\n",
    "    base_clients,           \n",
    "    num_tasks,\n",
    "    num_rounds_dict,\n",
    "    local_epochs,\n",
    "    lr,\n",
    "    device,\n",
    "    mode,\n",
    "    replay_ratio,\n",
    "    distil_coef,\n",
    "    EwcCoeff,\n",
    "    online_ewc_coeff,\n",
    "    replay_coeff,\n",
    "    fisher_matrices=None,\n",
    "    si_coeff=None,               # SI Œª\n",
    "    si_omegas=None,              # per-client per-param œâ\n",
    "    si_contributions=None,       # per-client per-param W accumulator\n",
    "    si_prev_params=None          # per-client Œ∏* (previous task snapshot)\n",
    "):\n",
    "\n",
    "    # TIP: This routine performs task-by-task federated updates with optional CF mitigations.\n",
    "    # It returns a list of immutable model snapshots, one after each task (for evaluation matrix P).\n",
    "\n",
    "    import torch, copy, numpy as np\n",
    "    import torch.nn as nn\n",
    "    from sklearn.cluster import KMeans\n",
    "    import torch._dynamo\n",
    "    torch._dynamo.disable()\n",
    "\n",
    "    # Reconstruct dims from base model\n",
    "    in_dim  = base_predictor.lstm.input_size\n",
    "    hid_dim = base_predictor.lstm.hidden_size\n",
    "    out_dim = base_predictor.fc.out_features\n",
    "\n",
    "    global_predictor = copy.deepcopy(base_predictor).to(device)\n",
    "    global_weights = global_predictor.state_dict()\n",
    "\n",
    "    clients = list(client_dataloaders[\"base_train\"].keys())\n",
    "    replay_buffers = {c: copy.deepcopy(client_buffers[c]) for c in clients}\n",
    "    predictors = {c: copy.deepcopy(global_predictor) for c in clients}\n",
    "    checkpoints = []\n",
    "\n",
    "    # Online EWC reference weights\n",
    "    ewc_reference_weights = {c: copy.deepcopy(base_predictor) for c in clients}\n",
    "    # HINT: For \"online_ewc\", these references are updated after each task using EMA Fisher.\n",
    "\n",
    "    for task_id in range(1, num_tasks + 1):\n",
    "        print(f\"\\nüîÅ Task {task_id}   mode = {mode}\")\n",
    "        task_train_key = f\"task_{task_id}_train\"\n",
    "\n",
    "        # --- SI: initialize per-task accumulators ---\n",
    "        if mode in [\"si\"]:\n",
    "            assert si_contributions is not None and si_omegas is not None and si_prev_params is not None, \\\n",
    "                \"Provide SI structures for SI modes.\"\n",
    "            for c in clients:\n",
    "                si_contributions.setdefault(c, {})\n",
    "                # zero tensors with correct shapes\n",
    "                for name, p in global_predictor.named_parameters():\n",
    "                    si_contributions[c][name] = torch.zeros_like(p, device=device)\n",
    "            # TIP: We re-zero per-task accumulators here; omegas and Œ∏* persist across tasks.\n",
    "\n",
    "        assert isinstance(num_rounds_dict, dict) and mode in num_rounds_dict\n",
    "        num_rounds = num_rounds_dict[mode]\n",
    "        print(f\"num_rounds = {num_rounds}\")\n",
    "\n",
    "        for r in range(num_rounds):\n",
    "            print(f\"üåê Communication Round {r+1}/{num_rounds}\")\n",
    "            local_weights = []\n",
    "\n",
    "            for c in clients:\n",
    "                predictor = copy.deepcopy(global_predictor).to(device)\n",
    "                old_predictor = copy.deepcopy(predictors[c]).to(device)\n",
    "                loader = client_dataloaders[task_train_key][c]\n",
    "                buffer = replay_buffers[c]\n",
    "\n",
    "                optimizer = torch.optim.Adam(predictor.parameters(), lr=lr)\n",
    "                predictor.train()\n",
    "\n",
    "                for _ in range(local_epochs):\n",
    "                    for X, y in loader:\n",
    "                        X, y = X.to(device), y.to(device)\n",
    "\n",
    "                        # ---- forward & base loss ----\n",
    "                        y_pred = predictor(X)\n",
    "                        reg_loss = nn.functional.mse_loss(y_pred, y)\n",
    "\n",
    "                        # ---- optional losses ----\n",
    "                        replay_loss = 0.0\n",
    "                        kd_loss = 0.0\n",
    "                        ewc_loss = 0.0\n",
    "                        ewc_loss_online = 0.0\n",
    "                        si_loss = 0.0\n",
    "\n",
    "                        if mode in [\"replay\"]:\n",
    "                            rX, rY = buffer.sample(X.shape[0])\n",
    "                            if rX is not None:\n",
    "                                rX, rY = rX.to(device), rY.to(device)\n",
    "                                rY_pred = predictor(rX)\n",
    "                                replay_loss = nn.functional.mse_loss(rY_pred, rY)\n",
    "\n",
    "                        if mode in [\"kd\"]:\n",
    "                            # KD teacher = previous local predictor for this client (stability signal)\n",
    "                            with torch.no_grad():\n",
    "                                teacher_output = old_predictor(X).detach()\n",
    "                            kd_loss = nn.functional.mse_loss(y_pred, teacher_output)\n",
    "\n",
    "                        if fisher_matrices is not None:\n",
    "                           \n",
    "                            for name, p in predictor.named_parameters():\n",
    "                                F = fisher_matrices.get(c, {}).get(name, None)\n",
    "                                if F is None: \n",
    "                                    continue\n",
    "                                if mode in [\"ewc\"]:\n",
    "                                    # Classic EWC: anchor to the client's base model (or global fallback)\n",
    "                                    ref_sd = base_clients[c].state_dict() if c in base_clients else base_predictor.state_dict()\n",
    "                                    p0 = ref_sd[name].to(device)\n",
    "                                    ewc_loss += (F * (p - p0).pow(2)).sum()\n",
    "                                if mode in [\"online_ewc\"]:\n",
    "                                    # Online EWC: anchor to moving reference after each task\n",
    "                                    pref = ewc_reference_weights[c].state_dict()[name].to(device)\n",
    "                                    ewc_loss_online += (F * (p - pref).pow(2)).sum()\n",
    "\n",
    "                        if mode in [\"si\"]:\n",
    "                            for name, p in predictor.named_parameters():\n",
    "                                if name in si_omegas[c]:\n",
    "                                    omega = si_omegas[c][name]\n",
    "                                    theta_star = si_prev_params[c][name]\n",
    "                                    si_loss += (omega * (p - theta_star).pow(2)).sum()\n",
    "\n",
    "                        # ---- backprop ----\n",
    "                        loss = (reg_loss\n",
    "                                + replay_coeff * replay_loss\n",
    "                                + distil_coef * kd_loss\n",
    "                                + EwcCoeff * ewc_loss\n",
    "                                + online_ewc_coeff * ewc_loss_online\n",
    "                                + (si_coeff or 0.0) * si_loss)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "\n",
    "                        # ====== SI contribution update (CORRECT) ======\n",
    "                        # snapshot Œ∏(t) BEFORE the optimizer step\n",
    "                        pre_step = {n: p.detach().clone() for n, p in predictor.named_parameters()}\n",
    "                        optimizer.step()\n",
    "                        # now Œ∏(t+1) is in-place; accumulate W += ŒîŒ∏ * (‚àíg)\n",
    "                        if mode in [\"si\"]:\n",
    "                            for name, p in predictor.named_parameters():\n",
    "                                g = p.grad  # grad from current loss (still present)\n",
    "                                if g is not None:\n",
    "                                    delta = p.detach() - pre_step[name]\n",
    "                                    si_contributions[c][name] += delta * (-g.detach())\n",
    "                        # ==============================================\n",
    "\n",
    "                local_weights.append(copy.deepcopy(predictor.state_dict()))\n",
    "\n",
    "            # FedAvg\n",
    "            newW = copy.deepcopy(local_weights[0])\n",
    "            for k in newW:\n",
    "                for i in range(1, len(local_weights)):\n",
    "                    newW[k] += local_weights[i][k]\n",
    "                newW[k] /= len(local_weights)\n",
    "\n",
    "            global_predictor.load_state_dict(newW)\n",
    "            for c in clients:\n",
    "                predictors[c] = copy.deepcopy(global_predictor)\n",
    "\n",
    "        # ---- Immutable checkpoint after finishing this task ----\n",
    "        snap = LSTMPredictor(in_dim, hid_dim, out_dim).to(device)\n",
    "        snap.load_state_dict({k: v.detach().clone() for k, v in global_predictor.state_dict().items()}, strict=True)\n",
    "        for p in snap.parameters(): p.requires_grad_(False)\n",
    "        checkpoints.append(snap)\n",
    "\n",
    "        # ---- Online EWC updates ----\n",
    "        if mode in [\"online_ewc\"] and fisher_matrices is not None:\n",
    "            for c in clients:\n",
    "                new_loader = client_dataloaders[task_train_key][c]\n",
    "                new_fisher = compute_ewc_fisher(predictors[c], new_loader, device=device)\n",
    "                for name, val in new_fisher.items():\n",
    "                    old_val = fisher_matrices[c].get(name, 0.0)\n",
    "                    fisher_matrices[c][name] = 0.9 * old_val + 0.1 * val\n",
    "                ewc_reference_weights[c] = copy.deepcopy(predictors[c])\n",
    "\n",
    "        # ---- SI omega update & Œ∏* snapshot (EXACT) ----\n",
    "        if mode in [\"si\"]:\n",
    "            xi = 1e-3\n",
    "            for c in clients:\n",
    "                final_params = predictors[c]\n",
    "                for name, p in final_params.named_parameters():\n",
    "                    delta_total = p.detach() - si_prev_params[c][name]  # Œ∏(T) - Œ∏*\n",
    "                    W = si_contributions[c][name]\n",
    "                    si_omegas[c][name] += W / (delta_total.pow(2) + xi)\n",
    "                    si_prev_params[c][name] = p.detach().clone()\n",
    "                # reset accumulators\n",
    "                for name in si_contributions[c]:\n",
    "                    si_contributions[c][name].zero_()\n",
    "\n",
    "        # ---- K-means replay update ----\n",
    "        for c in clients:\n",
    "            if getattr(replay_buffers[c], \"capacity\", 0) <= 0:\n",
    "                continue\n",
    "            loader = client_dataloaders[task_train_key][c]\n",
    "            all_X, all_y = [], []\n",
    "            global_predictor.eval()\n",
    "            with torch.no_grad():\n",
    "                for X, y in loader:\n",
    "                    all_X.append(X.cpu()); all_y.append(y.cpu())\n",
    "            if not all_X:\n",
    "                continue\n",
    "            all_X = torch.cat(all_X, 0); all_y = torch.cat(all_y, 0)\n",
    "            X_embed = all_X.view(all_X.size(0), -1).numpy()\n",
    "            n_total = len(all_X); n_sel = int(replay_ratio * n_total)\n",
    "            if n_sel <= 0: \n",
    "                continue\n",
    "            if n_sel >= len(X_embed):\n",
    "                sel = np.arange(n_total)\n",
    "            else:\n",
    "                km = KMeans(n_clusters=n_sel, random_state=42).fit(X_embed)\n",
    "                centers, labels = km.cluster_centers_, km.labels_\n",
    "                sel = []\n",
    "                for k in range(n_sel):\n",
    "                    ids = np.where(labels == k)[0]\n",
    "                    if ids.size == 0: continue\n",
    "                    D = np.linalg.norm(X_embed[ids] - centers[k], axis=1)\n",
    "                    sel.append(ids[np.argmin(D)])\n",
    "            for i in sel:\n",
    "                x_i, y_i = all_X[i], all_y[i]\n",
    "                if x_i.ndim == 2:\n",
    "                    replay_buffers[c].add(x_i, y_i)\n",
    "                elif x_i.ndim == 3:\n",
    "                    for j in range(x_i.shape[0]):\n",
    "                        replay_buffers[c].add(x_i[j], y_i[j])\n",
    "\n",
    "    return checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa78907-619c-4806-9765-02c8b8907ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6afc3dff-ae25-402e-8b20-74a759a61105",
   "metadata": {},
   "source": [
    "<h1>9. Run Ablation  & Evaluate   </h1>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f853c-8cf8-457f-9e2b-a2e75f3b8c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combined_ablation(\n",
    "    MODES,\n",
    "    REPLAY_RATIO,\n",
    "    KD_COEFF,\n",
    "    EWC_COEFF,\n",
    "    ONLINE_EWC_COEFF,\n",
    "    REPLAY_COEFF,\n",
    "    FISHER_MATRICES,\n",
    "    SI_OMEGAS,\n",
    "    SI_CONTRIBUTIONS,\n",
    "    SI_PREV_PARAMS,\n",
    "    SI_COEFF,\n",
    "    CLIENT_DATALOADERS,\n",
    "    BASE_MODEL,\n",
    "    BASE_CLIENTS,\n",
    "    NUM_TASKS,\n",
    "    NUM_ROUNDS_CL,\n",
    "    LOCAL_EPOCHS,\n",
    "    LR,\n",
    "    DEVICE\n",
    "):\n",
    "    import copy, time\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "    checkpoints_per_mode = {}\n",
    "\n",
    "    cpu_times = {}\n",
    "\n",
    "    saved_base_weights = BASE_MODEL.state_dict()\n",
    "\n",
    "    for mode in MODES:\n",
    "        print(f\"\\n======= Running Mode: {mode} =======\")\n",
    "\n",
    "        # SI payloads per mode\n",
    "        if mode in [\"si\", \"kd+si\", \"si+replay\", \"si+ewc\"]:\n",
    "            si_omegas_mode = copy.deepcopy(SI_OMEGAS)\n",
    "            si_prev_params_mode = copy.deepcopy(SI_PREV_PARAMS)\n",
    "            si_contributions_mode = {}\n",
    "            for c in CLIENT_DATALOADERS[\"base_train\"]:\n",
    "                ref = BASE_CLIENTS.get(c, BASE_MODEL)   # <-- fallback ensures shapes exist\n",
    "                si_contributions_mode[c] = {name: torch.zeros_like(p) for name, p in ref.named_parameters()}\n",
    "        else:\n",
    "            si_omegas_mode = si_prev_params_mode = si_contributions_mode = None\n",
    "\n",
    "\n",
    "        # Replay buffers per mode (use your current name `initial_buffers`)\n",
    "        mode_replay_buffers = {}\n",
    "        for c in CLIENT_DATALOADERS[\"base_train\"]:\n",
    "            if mode in [\"replay\", \"ewc+replay\", \"kd+replay\", \"si+replay\"]:\n",
    "                original = initial_buffers[c]\n",
    "                buf = ReplayBuffer(capacity=original.capacity)\n",
    "                for x, y in original.buffer:\n",
    "                    if isinstance(x, torch.Tensor) and x.ndim == 2:\n",
    "                        buf.add(x, y)\n",
    "            else:\n",
    "                buf = ReplayBuffer(capacity=0)\n",
    "            mode_replay_buffers[c] = buf\n",
    "\n",
    "        # Reinit base for this mode\n",
    "        base_copy = LSTMPredictor(\n",
    "            input_dim=BASE_MODEL.lstm.input_size,\n",
    "            hidden_dim=BASE_MODEL.lstm.hidden_size,\n",
    "            output_dim=BASE_MODEL.fc.out_features\n",
    "        ).to(DEVICE)\n",
    "        base_copy.load_state_dict(saved_base_weights)\n",
    "\n",
    "        rounds_config = {\n",
    "            \"Naive\": NUM_ROUNDS_CL,\n",
    "            \"replay\": NUM_ROUNDS_CL,\n",
    "            \"kd\": NUM_ROUNDS_CL,\n",
    "            \"online_ewc\": NUM_ROUNDS_CL,\n",
    "            \"ewc\": NUM_ROUNDS_CL,\n",
    "            # keep combos for future use\n",
    "            \"si\": NUM_ROUNDS_CL, \"ewc+replay\": NUM_ROUNDS_CL, \"kd+si\": NUM_ROUNDS_CL,\n",
    "            \"si+ewc\": NUM_ROUNDS_CL, \"kd+replay\": NUM_ROUNDS_CL, \"ewc+kd\": NUM_ROUNDS_CL\n",
    "        }\n",
    "\n",
    "        kd_for_mode = KD_COEFF if mode in [\"kd\"] else 0.0\n",
    "\n",
    "        t0 = time.time()\n",
    "        checkpoints = continual_learning_hybrid(\n",
    "            client_dataloaders=CLIENT_DATALOADERS,\n",
    "            client_buffers=mode_replay_buffers,\n",
    "            base_predictor=base_copy,\n",
    "            base_clients=BASE_CLIENTS,\n",
    "            num_tasks=NUM_TASKS,\n",
    "            num_rounds_dict=rounds_config,\n",
    "            local_epochs=LOCAL_EPOCHS,\n",
    "            lr=LR,\n",
    "            device=DEVICE,\n",
    "            mode=mode,\n",
    "            replay_ratio=REPLAY_RATIO,\n",
    "            distil_coef=kd_for_mode,\n",
    "            EwcCoeff=EWC_COEFF,\n",
    "            online_ewc_coeff=ONLINE_EWC_COEFF,\n",
    "            replay_coeff=REPLAY_COEFF,\n",
    "            fisher_matrices=FISHER_MATRICES if mode in [\"ewc\", \"online_ewc\"] else None,\n",
    "            si_coeff=SI_COEFF,\n",
    "            si_omegas=si_omegas_mode,\n",
    "            si_contributions=si_contributions_mode,\n",
    "            si_prev_params=si_prev_params_mode\n",
    "        )\n",
    "        cpu_times[mode] = time.time() - t0\n",
    "        checkpoints_per_mode[mode] = checkpoints\n",
    "\n",
    "\n",
    "\n",
    "    return checkpoints_per_mode, cpu_times\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_rmse_matrix_and_metrics(ablation_checkpoints, client_dataloaders, cpu_times, device=DEVICE):\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    final_metrics = {}\n",
    "\n",
    "    for mode, checkpoints in ablation_checkpoints.items():\n",
    "        N = len(checkpoints)\n",
    "        # Legacy: zero-init matrices (unseen cells stay 0 instead of NaN)\n",
    "        rmse_matrix = np.zeros((N, N), dtype=float)\n",
    "        mae_matrix  = np.zeros((N, N), dtype=float)\n",
    "\n",
    "        # Fill lower triangle\n",
    "        for i, predictor in enumerate(checkpoints):\n",
    "            predictor.eval()\n",
    "            for j in range(i + 1):\n",
    "                task_key = f\"task_{j+1}_test\"\n",
    "                preds, trues = [], []\n",
    "                for _, loader in client_dataloaders[task_key].items():\n",
    "                    with torch.no_grad():\n",
    "                        for X, y in loader:\n",
    "                            X, y = X.to(device), y.to(device)\n",
    "                            preds.append(predictor(X).cpu().numpy().ravel())\n",
    "                            trues.append(y.cpu().numpy().ravel())\n",
    "                if preds and trues:\n",
    "                    P = np.concatenate(preds); Y = np.concatenate(trues)\n",
    "                    # Guard just in case\n",
    "                    if P.size and Y.size and np.isfinite(P).all() and np.isfinite(Y).all():\n",
    "                        rmse_matrix[i, j] = np.sqrt(mean_squared_error(Y, P))\n",
    "                        mae_matrix[i, j]  = mean_absolute_error(Y, P)\n",
    "                # else: leave zeros (legacy behavior)\n",
    "\n",
    "        # === Legacy metrics (match your original notebook) ===\n",
    "        # AvgPerf over tasks 1..N-1\n",
    "        last_row_rmse = rmse_matrix[-1, :-1] if N > 1 else rmse_matrix[-1:, -1:]\n",
    "        avg_perf = float(np.mean(last_row_rmse))\n",
    "\n",
    "        # AF over first N-1 tasks: P_{N,j} - P_{j,j}\n",
    "        diag_rmse = np.diag(rmse_matrix)\n",
    "        if N > 1:\n",
    "            af = float(np.mean(rmse_matrix[-1, :-1] - diag_rmse[:-1]))\n",
    "        else:\n",
    "            af = 0.0\n",
    "\n",
    "        # AP (legacy: diagonal mean; if you want strictly legacy, this matched your prints)\n",
    "        ap = float(np.mean(diag_rmse))\n",
    "\n",
    "        final_metrics[mode] = {\n",
    "            \"rmse_matrix\": rmse_matrix,\n",
    "            \"mae_matrix\": mae_matrix,\n",
    "            \"avg_perf\": avg_perf,\n",
    "            \"avg_forget_best\": float('nan'),     # unused in your table\n",
    "            \"avg_forget_taskwise\": af,\n",
    "            \"avg_plasticity\": ap,\n",
    "            \"last_model_rmse\": rmse_matrix[-1, :],\n",
    "            \"last_model_mae\":  mae_matrix[-1, :],\n",
    "            \"forgetting_vector_best\": np.array([]),\n",
    "            \"forgetting_vector_taskwise\": (rmse_matrix[-1, :-1] - diag_rmse[:-1]) if N > 1 else np.array([]),\n",
    "            \"cpu_time\": cpu_times[mode],\n",
    "        }\n",
    "\n",
    "        print(f\"üî¢ [{mode}] AvgPerf={avg_perf:.6f} | AF={af:.6f} | AP={ap:.6f}\")\n",
    "\n",
    "    return final_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== Run Ablation Study on different modes =====\n",
    "set_all_seeds(42)\n",
    "MODES = [\"Naive\",\"replay\", \"kd\", \"online_ewc\", \"ewc\", \"si\"]\n",
    "\n",
    "ablation_checkpoints, cpu_times = run_combined_ablation(\n",
    "    MODES=MODES,\n",
    "    REPLAY_RATIO=REPLAY_RATIO,\n",
    "    KD_COEFF=KD_COEFF,\n",
    "    EWC_COEFF=EWC_COEFF,\n",
    "    ONLINE_EWC_COEFF=ONLINE_EWC_COEFF,\n",
    "    REPLAY_COEFF=REPLAY_COEFF,\n",
    "    FISHER_MATRICES=fisher,\n",
    "    SI_OMEGAS=si_omegas,\n",
    "    SI_CONTRIBUTIONS=si_W,\n",
    "    SI_PREV_PARAMS=si_prev,\n",
    "    SI_COEFF=SI_COEFF,\n",
    "    CLIENT_DATALOADERS=CLIENT_DATALOADERS,\n",
    "    BASE_MODEL=BASE_MODEL,\n",
    "    BASE_CLIENTS=BASE_CLIENTS,\n",
    "    NUM_TASKS=NUM_TASKS,\n",
    "    NUM_ROUNDS_CL=NUM_ROUNDS_CL,\n",
    "    LOCAL_EPOCHS=LOCAL_EPOCHS,\n",
    "    LR=LR,\n",
    "    DEVICE=DEVICE\n",
    ")\n",
    "\n",
    "# Evaluate with your original logic\n",
    "final_metrics = evaluate_rmse_matrix_and_metrics(\n",
    "    ablation_checkpoints=ablation_checkpoints,\n",
    "    client_dataloaders=CLIENT_DATALOADERS,\n",
    "    cpu_times=cpu_times,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    mode: {\n",
    "        \"AvgForgetting\": metrics[\"avg_forget_taskwise\"] * 1000,  # your scaling\n",
    "        \"AvgPlasticity\": metrics[\"avg_plasticity\"],\n",
    "        \"AvgPerformance\": metrics[\"avg_perf\"],\n",
    "        \"CPUTime(s)\": metrics[\"cpu_time\"]\n",
    "    }\n",
    "    for mode, metrics in final_metrics.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\nüìä Final Results Table:\")\n",
    "print(summary_df.round(7).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c9daea-e824-419c-86c3-2dd69d820c7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190eab5-2d0a-489c-878c-77bfecf9c5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
